"""
æ”¹é€²çš„ PyTorch è¨“ç·´è…³æœ¬ï¼Œé©ç”¨æ–¼ ViViT æ¨¡å‹åœ¨ HMDB51 æ•¸æ“šé›†ä¸Šçš„è¨“ç·´ã€‚
æ­¤ç‰ˆæœ¬æ•´åˆäº†ï¼š
  - åŸå§‹æ•¸æ“šèˆ‡æ•¸æ“šå¢å¼· (augmented) å…©ç¨®è¨“ç·´æ¨¡å¼ï¼Œ
  - AMP è‡ªå‹•æ··åˆç²¾åº¦ (Automatic Mixed Precision) èˆ‡æ¢¯åº¦è£å‰ªï¼Œ
  - CosineAnnealingLR å­¸ç¿’ç‡èª¿åº¦å™¨ï¼Œ
  - checkpoint è¼‰å…¥èˆ‡å„²å­˜ï¼Œ
"""
import os
import time
import json
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from torch.utils.data import ConcatDataset, Subset
import random
from transformers import get_cosine_schedule_with_warmup
import numpy as np

# å•Ÿç”¨ cudnn å„ªåŒ–
torch.backends.cudnn.benchmark = True

# å¾ model.py è¼‰å…¥å„æ¨¡å‹ï¼ˆè«‹ç¢ºä¿æ¨¡å‹åç¨±èˆ‡ä½ çš„è¨“ç·´éœ€æ±‚ä¸€è‡´ï¼‰
from model import model1, ViViT_Factorized, ViViT_Factorized_selfAttention, ViViT_Factorized_DotProduct
# å¾ dataset.py è¼‰å…¥ HMDB51Datasetï¼ˆæ­¤ Dataset éœ€æ”¯æ´ mode åƒæ•¸ï¼šraw æˆ– augmentedï¼‰
from dataset import HMDB51Dataset

# å»ºç«‹å…¨åŸŸçš„ AMP GradScaler
scaler = torch.cuda.amp.GradScaler()

def get_model(model_name: str):
    """
    æ ¹æ“šæ¨¡å‹åç¨±è¿”å›å°æ‡‰çš„æ¨¡å‹å¯¦ä¾‹ã€‚
    """
    model_dict = {
        'model1': model1(
            in_channels=3, embed_dim=128, patch_size=16, tubelet_size=2,
            num_heads=8, mlp_dim=512, num_layers=6, num_classes=51,
            num_frames=32, img_size=224
        ),
        'vivit_factorized': ViViT_Factorized(
            in_channels=3, embed_dim=64, patch_size=16, tubelet_size=2,
            num_heads=8, mlp_dim=384, num_layers_spatial=4, num_layers_temporal=4,
            num_classes=51, num_frames=32, img_size=224, droplayer_p=0.1
        ),
        'vivit_factorized_selfattention': ViViT_Factorized_selfAttention(
            in_channels=3, embed_dim=64, patch_size=16, tubelet_size=2,
            num_heads=8, mlp_dim=64 * 4, num_layers=6, num_classes=51,
            num_frames=32, img_size=224, dropout=0.6, droplayer_p=0.2
        ),
        'vivit_factorized_dotproduct': ViViT_Factorized_DotProduct(
            in_channels=3, embed_dim=64, patch_size=16, tubelet_size=2,
            num_heads=8, mlp_dim=64 * 4, num_layers=4, num_classes=51,
            num_frames=32, img_size=224,
        ),
    }
    if model_name not in model_dict:
        raise ValueError(f"âŒ éŒ¯èª¤ï¼šä¸æ”¯æ´çš„æ¨¡å‹ `{model_name}`ï¼") # ç›´æ¥è®“ç¨‹å¼åœæ­¢åŸ·è¡Œ
    return model_dict[model_name]

def train_one_epoch(model: nn.Module,
                    train_loader: DataLoader,
                    criterion: nn.Module, # loss function
                    optimizer: optim.Optimizer,
                    scheduler, # å­¸ç¿’ç‡èª¿æ•´å™¨
                    device: torch.device, # cpuã€gpu
                    epoch: int, # ç•¶å‰ epoch ç·¨è™Ÿ
                    accumulation_steps: int = 4) -> (float, float):
    """
    è¨“ç·´ä¸€å€‹ epochï¼Œæ”¯æ´æ¢¯åº¦ç´¯ç©ï¼Œä¸¦ä¸”åªåŒ…å«åˆä½µå¾Œçš„æ•¸æ“šé›† `train_loader`ã€‚
    """
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    start_time = time.time()

    optimizer.zero_grad()  # ä¿è­‰æ¯å€‹ epoch åˆå§‹æ™‚æ¢¯åº¦æ­¸é›¶

    # è¨“ç·´å–®ä¸€æ··åˆæ•¸æ“šé›†
    print(f"\nğŸš€ Epoch {epoch} - è¨“ç·´æ··åˆæ•¸æ“šé›† ")
    # batch_idx : batch index, (inputs, labels) : (å½±ç‰‡, åˆ†é¡çµæœ)
    # train_loader : DataLoader(train_dataset, batch_size, shuffle=True)
    # å¾train_datasetéš¨æ©Ÿä¸€å€‹ batch (åŒ…å« batch_size å€‹ frame)ï¼Œä¸¦è¿”å›ç´¢å¼•batch_idxã€è³‡æ–™(inputs, labels)
    # tqdmé¡¯ç¤ºé€²åº¦æ¢ã€desc:é€²åº¦æ¢æ¨™é¡Œã€leave:ç•™ä¸‹é€²åº¦æ¢
    for batch_idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch}", leave=True)):
        inputs, labels = inputs.to(device), labels.to(device) # å°‡label inputæ¬ç§»åˆ° GPU æˆ– CPU
        with torch.cuda.amp.autocast():  # å•Ÿç”¨ AMP æ··åˆç²¾åº¦
            outputs = model(inputs) # forward pass

            # criterion(outputs, labels):è¨ˆç®—ç•¶å‰å‡½æ•¸æå¤±å€¼ã€accumulation_steps=4ï¼Œè¡¨ç¤ºè¦ç´¯ç© 4 å€‹ batch çš„æ¢¯åº¦å¾Œå†æ›´æ–°ä¸€æ¬¡
            loss = criterion(outputs, labels) / accumulation_steps

        # scale(loss)å…ˆæ”¾å¤§ loss (é¿å… FP16 æ•¸å€¼ç²¾åº¦å•é¡Œ)
        # backwardä¸æœƒé¦¬ä¸Šæ¸…é™¤æ¢¯åº¦ï¼Œæœƒç´¯ç©åœ¨model.parameters()
        scaler.scale(loss).backward()

        if (batch_idx + 1) % accumulation_steps == 0: # åªæœ‰æ˜¯accumulation_stepsçš„å€æ•¸æ‰æ›´æ–°æ¢¯åº¦
            scaler.unscale_(optimizer) #å–æ¶ˆscale(loss)çš„æ”¾å¤§

            #é™åˆ¶æ‰€æœ‰åƒæ•¸çš„L2 NORMä¸è¶…é3ã€‚å‘é‡[X1,X2]å‰‡æ­¤å‘é‡çš„NORMæ˜¯(X1^2+X2^2)^1/2
            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0) # æ¢¯åº¦è£å‰ªç”¨ä¾†é¿å…æ¢¯åº¦çˆ†ç‚¸

            # æ›´æ–°æ¢¯åº¦
            scaler.step(optimizer)
            scaler.update() # èª¿æ•´ AMP ç¸®æ”¾å› å­

            # æ¸…é™¤æ¢¯åº¦
            optimizer.zero_grad()

        _, preds = torch.max(outputs, 1) # æ²¿è‘—ç¶­åº¦ 1(é¡åˆ¥ç¶­åº¦)æ‰¾å‡ºæœ€å¤§å€¼(é æ¸¬çš„é¡åˆ¥)
        correct += preds.eq(labels).sum().item() # è¨ˆç®—é æ¸¬æ­£ç¢ºçš„æ¨£æœ¬æ•¸
        total += labels.size(0) #å–å¾—ç•¶å‰batchçš„æ¨£æœ¬æ•¸ï¼Œç´¯åŠ åˆ°total
        total_loss += loss.item() * accumulation_steps  # losséœ€ä¹˜å›accumulation_steps

    avg_loss = total_loss / len(train_loader)  # ç¢ºä¿ Loss å¹³å‡è¨ˆç®—
    accuracy = correct / total * 100.0
    epoch_time = time.time() - start_time
    print(f"\nğŸ”¥ Epoch {epoch}: Avg Loss {avg_loss:.4f}, Accuracy {accuracy:.2f}%, Time {epoch_time:.2f} sec")

    scheduler.step()
    return avg_loss, accuracy

def evaluate(model: nn.Module, test_loader: DataLoader, criterion: nn.Module, device: torch.device) -> (float, float):
    """
    è©•ä¼°æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾ï¼Œè¨ˆç®— Loss èˆ‡ Accuracyã€‚

    Args:
        model (nn.Module)       : å·²è¨“ç·´å¥½çš„æ¨¡å‹
        test_loader (DataLoader) : æ¸¬è©¦é›†çš„ DataLoader
        criterion (nn.Module)    : æå¤±å‡½æ•¸ (å¦‚ CrossEntropyLoss)
        device (torch.device)    : é‹ç®—è¨­å‚™ (CPU/GPU)

    Returns:
        avg_loss (float) : æ¸¬è©¦é›†çš„å¹³å‡ Loss
        accuracy (float) : æ¸¬è©¦é›†çš„æº–ç¢ºç‡ (%)
    """

    model.eval()  # è¨­ç½®æ¨¡å‹ç‚ºã€Œè©•ä¼°æ¨¡å¼ã€ (Evaluation Mode)ï¼Œé—œé–‰ Dropout/BatchNorm

    total_loss = 0.0  # ç´¯ç© Loss
    correct = 0  # é æ¸¬æ­£ç¢ºçš„æ¨£æœ¬æ•¸
    total = 0  # æ¸¬è©¦é›†ç¸½æ¨£æœ¬æ•¸

    with torch.no_grad():  # é—œé–‰æ¢¯åº¦è¨ˆç®—ï¼Œç¯€çœè¨˜æ†¶é«”èˆ‡è¨ˆç®—è³‡æº
        for inputs, labels in test_loader:  # éæ­·æ¸¬è©¦é›†æ¯å€‹ batch
            inputs, labels = inputs.to(device), labels.to(device)  # å°‡æ•¸æ“šæ¬ç§»åˆ° GPU / CPU

            outputs = model(inputs)  # Forward Pass
            loss = criterion(outputs, labels)  # è¨ˆç®— Loss
            total_loss += loss.item()  # ç´¯ç© Loss

            _, preds = torch.max(outputs, 1)  # å–å¾—é æ¸¬çš„é¡åˆ¥ (argmax)
            correct += preds.eq(labels).sum().item()  # è¨ˆç®—æ­£ç¢ºé æ¸¬æ•¸é‡
            total += labels.size(0)  # ç´¯ç©æ¨£æœ¬ç¸½æ•¸

    avg_loss = total_loss / len(test_loader)  # è¨ˆç®—å¹³å‡ Loss
    accuracy = correct / total * 100.0  # è¨ˆç®—æº–ç¢ºç‡ (%)

    print(f"âœ… æ¸¬è©¦é›†: Loss {avg_loss:.4f}, Accuracy {accuracy:.2f}%")  # ğŸ”¹ é¡¯ç¤ºè©•ä¼°çµæœ

    return avg_loss, accuracy  # ğŸ”¥ å›å‚³æ¸¬è©¦é›†çš„å¹³å‡ Loss èˆ‡æº–ç¢ºç‡

def get_args():
    parser = argparse.ArgumentParser(description="Train ViViT Model on HMDB51")
    parser.add_argument("--epochs", type=int, default=50, help="è¨“ç·´ Epoch æ•¸")
    parser.add_argument("--batch_size", type=int, default=8, help="Batch å¤§å°")
    parser.add_argument("--lr", type=float, default=3e-4, help="åˆå§‹å­¸ç¿’ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.05, help="æ¬Šé‡è¡°æ¸› (L2 æ­£å‰‡åŒ–)")
    parser.add_argument("--train_data", type=str, default="preprocessed_data_pt/train", help="è¨“ç·´æ•¸æ“šé›†è·¯å¾‘")
    parser.add_argument("--test_data", type=str, default="preprocessed_data_pt/test", help="æ¸¬è©¦æ•¸æ“šé›†è·¯å¾‘")
    parser.add_argument("--save_dir", type=str, default="checkpoints", help="å„²å­˜æ¨¡å‹çš„è³‡æ–™å¤¾")
    parser.add_argument("--resume", type=str, default=None, help="è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹ (.pth)")
    parser.add_argument("--scheduler", type=str, default="cosine", choices=["plateau", "cosine"], help="é¸æ“‡å­¸ç¿’ç‡èª¿åº¦å™¨")
    parser.add_argument("--model_name", type=str, default="vivit_factorized_selfattention",
                        choices=["model1", "vivit_factorized", "vivit_factorized_selfattention", "vivit_factorized_dotproduct"],
                        help="é¸æ“‡è¦è¨“ç·´çš„æ¨¡å‹")
    return parser.parse_args()


def save_model(model: nn.Module, optimizer: optim.Optimizer, epoch: int, save_dir: str = "checkpoints"):
    """å„²å­˜æ¨¡å‹æ¬Šé‡èˆ‡è¨“ç·´ç‹€æ…‹"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_path = os.path.join(save_dir, f"vivit_epoch_{epoch}.pth")
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict()
    }, save_path)
    print(f"âœ… æ¨¡å‹å·²å„²å­˜è‡³ {save_path}")

def main():
    args = get_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è£ç½®: {device}")

    writer = SummaryWriter(log_dir="./logs")

    # è¼‰å…¥æ•¸æ“šé›†ï¼šè¨“ç·´æ•¸æ“šåˆ†ç‚º raw èˆ‡ augmented å…©éƒ¨åˆ†
    print("ğŸ“‚ æ­£åœ¨è¼‰å…¥è¨“ç·´æ•¸æ“šé›†...")
    train_dataset_raw = HMDB51Dataset(root_dir=args.train_data, mode="raw")
    train_dataset_aug = HMDB51Dataset(root_dir=args.train_data, mode="augmented")

    # è¨ˆç®—å¢å¼·æ•¸æ“šæ‡‰è©²å–å¤šå°‘ï¼ˆ10%ï¼‰
    num_aug_samples = int(len(train_dataset_raw) * 0.1)  # 10% å¢å¼·æ•¸æ“š
    aug_indices = random.sample(range(len(train_dataset_aug)), num_aug_samples) #éš¨æ©ŸæŠ½å–num_aug_sampleså€‹æ•¸æ“š
    train_dataset_aug_sampled = Subset(train_dataset_aug, aug_indices) #å»ºç«‹æŠ½å–å¾Œçš„å­æ•¸æ“š

    # åˆä½µæ•¸æ“šé›†ï¼ˆå…¨éƒ¨åŸå§‹ + 10% å¢å¼·ï¼‰
    train_dataset_combined = ConcatDataset([train_dataset_raw, train_dataset_aug_sampled])

    # å‰µå»ºæ–°çš„ DataLoader
    train_loader = DataLoader(
        train_dataset_combined,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=6,
        pin_memory=True
    )

    print("ğŸ“‚ æ­£åœ¨è¼‰å…¥æ¸¬è©¦æ•¸æ“šé›†...")
    test_dataset = HMDB51Dataset(root_dir=args.test_data, mode="raw")
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=6, pin_memory=True)

    # ğŸš€ ä½¿ç”¨ `get_model()` ä¾†è¼‰å…¥æ¨¡å‹
    print(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {args.model_name}")
    model = get_model(args.model_name).to(device)

    # å®šç¾©æå¤±å‡½æ•¸èˆ‡å„ªåŒ–å™¨
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # ä½¿ç”¨ Label Smoothing
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay) #åƒæ•¸ã€å­¸ç¿’ç‡ã€æ­£è¦åŒ–

    # è¨­å®š Warmup æ­¥æ•¸ï¼Œé€šå¸¸ç‚ºç¸½ epochs çš„ 10%
    num_warmup_steps = int(args.epochs * 0.1)

    if args.scheduler == "plateau":
        scheduler = ReduceLROnPlateau(optimizer, "min", patience=3, factor=0.5)
    else:
        scheduler = get_cosine_schedule_with_warmup(
            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=args.epochs
        )
    start_epoch = 1
    if args.resume and os.path.exists(args.resume):
        print(f"ğŸ”„ è¼‰å…¥ checkpoint: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)#è¼‰å…¥.pth
        model.load_state_dict(checkpoint["model_state_dict"])#è¼‰å…¥æ¨¡å‹åƒæ•¸(æ¬Šé‡ã€åå·®)
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])#è¼‰å…¥å„ªåŒ–å™¨(å­¸ç¿’ç‡ã€æ¢¯åº¦å‹•é‡)
        start_epoch = checkpoint["epoch"] + 1
        print(f"âœ… å¾ Epoch {start_epoch} ç¹¼çºŒè¨“ç·´")
    else:
        print("âŒ æœªæ‰¾åˆ° checkpointï¼Œå¾ Epoch 1 é–‹å§‹è¨“ç·´")

    os.makedirs(args.save_dir, exist_ok=True)#åœ¨ç•¶å‰è·¯å¾‘å»ºç«‹save_dirè³‡æ–™å¤¾

    for epoch in range(start_epoch, args.epochs + 1):
        print(f"\nğŸš€ é–‹å§‹è¨“ç·´ Epoch {epoch}/{args.epochs}")
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch)

        # æ¯ 5 å€‹ epoch å„²å­˜ä¸€æ¬¡ checkpointã€å’Œè©•ä¼°æ¸¬è©¦é›†
        if epoch % 5 == 0 or epoch == args.epochs:
            save_model(model, optimizer, epoch, args.save_dir)
            print("\nğŸ“Š è©•ä¼°æ¸¬è©¦é›†...")
            test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    print(f"\nğŸ‰ è¨“ç·´å®Œæˆï¼")
    writer.close()

if __name__ == "__main__":
    main()

import os
import torch
import cv2
import numpy as np
import argparse
import json
from torchvision import transforms
from tqdm import tqdm
from train import get_model


# å½±ç‰‡é è™•ç†å‡½æ•¸
def preprocess_video(video_path, frame_count=32, frame_size=(224, 224)):
    """
    è®€å– .avi å½±ç‰‡ä¸¦è½‰æ›ç‚º ViViT å¯è™•ç†çš„æ ¼å¼ (C, T, H, W)ã€‚
    """
    cap = cv2.VideoCapture(video_path)
    frames = []

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # å–å¾—å½±ç‰‡çš„ç¸½å¹€æ•¸
    frame_indices = np.linspace(0, total_frames - 1, frame_count).astype(int)  # å‡å‹»å–æ¨£ frame_count å¹€

    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # è·³è½‰åˆ°ç¬¬ idx å¹€
        ret, frame = cap.read()  # è®€å–è©²å¹€
        if not ret:
            break  # è‹¥è®€å–å¤±æ•—ï¼Œå‰‡è·³å‡º

        frame = cv2.resize(frame, frame_size)  # èª¿æ•´å½±åƒå¤§å° (224x224)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # è½‰æ› BGR â†’ RGB
        frames.append(frame)  # å­˜å…¥ frames é™£åˆ—

    cap.release()

    while len(frames) < frame_count:
        frames.append(frames[-1])  # è‹¥å¹€æ•¸ä¸è¶³ frame_countï¼Œå‰‡å¡«å……æœ€å¾Œä¸€å¹€

    transform = transforms.Compose([
        transforms.ToTensor(),  # è½‰æ›ç‚º Tensorï¼Œ(H, W, C) â†’ (C, H, W)
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # æ¨™æº–åŒ–
    ])

    frames = torch.stack([transform(frame) for frame in frames])  # æŠŠframes(C, H, W)å †ç–Šæˆ(T, C, H, W)
    frames = frames.permute(1, 0, 2, 3)  # è½‰æ›ç‚º (C, T, H, W)

    return frames.unsqueeze(0)  # å¢åŠ  batch ç¶­åº¦ (1, C, T, H, W)

# å½±ç‰‡åˆ†é¡é æ¸¬
def predict(model, video_tensor, idx_to_class, top_k=5):
    """
    ä½¿ç”¨ ViViT æ¨¡å‹é€²è¡Œæ¨ç†ï¼Œè¿”å› Top-K çµæœ (é¡åˆ¥åç¨±)ã€‚
    idx_to_class ex:{0: "brush_hair", 1: "cartwheel", 2: "catch", 3: "chew", ...}

    """
    model.eval()
    with torch.no_grad():  # é—œé–‰æ¢¯åº¦è¨ˆç®—
        outputs = model(video_tensor)  # æ¨¡å‹è¼¸å‡º logits
        probs = torch.nn.functional.softmax(outputs, dim=1)  # è½‰æ›ç‚ºæ©Ÿç‡(0åˆ°1ä¹‹é–“)
        top_probs, top_classes = torch.topk(probs, top_k, dim=1)  # å–å¾—å‰ké«˜çš„æ©Ÿç‡èˆ‡é¡åˆ¥

    # å–å¾—æ¨¡å‹é æ¸¬çš„ Top-K é¡åˆ¥ç´¢å¼• (Tensor)
    top_classes = top_classes.squeeze()  # ç§»é™¤å¤šé¤˜ç¶­åº¦(ç¶­åº¦ç‚º1 å³batch size)ï¼Œä½¿å…¶è®Šæˆä¸€ç¶­å¼µé‡
    top_classes_list = top_classes.tolist()  # è½‰æ›ç‚º Python åˆ—è¡¨

    # å°‡é¡åˆ¥ç´¢å¼•è½‰æ›ç‚ºå°æ‡‰çš„é¡åˆ¥åç¨±
    top_class_names = []  # å­˜æ”¾é¡åˆ¥åç¨±
    for idx in top_classes_list:
        class_name = idx_to_class.get(idx, "Unknown")  # é€éå­—å…¸æŸ¥æ‰¾é¡åˆ¥åç¨±
        top_class_names.append(class_name)  # åŠ å…¥çµæœåˆ—è¡¨

    return top_class_names, top_probs.squeeze().tolist()

def get_args():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ ViViT æ¨¡å‹é€²è¡Œå½±ç‰‡åˆ†é¡")
    parser.add_argument("video_path", type=str, help="è¼¸å…¥ AVI å½±ç‰‡çš„è·¯å¾‘")
    parser.add_argument("model_path", type=str, help="å·²è¨“ç·´æ¨¡å‹çš„æ¬Šé‡æ–‡ä»¶ (.pth)")
    parser.add_argument("--model_name", type=str, default="vivit_factorized_dotproduct",
                        choices=["model1", "vivit_factorized", "vivit_factorized_selfattention",
                                 "vivit_factorized_dotproduct"],
                        help="é¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹")
    parser.add_argument("--top_k", type=int, default=5, help="è¦é¡¯ç¤ºçš„ Top-K çµæœ")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
                        help="é‹è¡Œè£ç½® (cpu æˆ– cuda)")
    parser.add_argument("--strict", action="store_true", help="æ˜¯å¦å¼·åˆ¶åŒ¹é… state_dict éµå€¼")

    return parser.parse_args()

# ä¸»ç¨‹å¼
def main():
    args = get_args()
    device = torch.device(args.device)
    print(f"ğŸš€ é‹è¡Œè£ç½®: {device}")

    # è¼‰å…¥ ViViT æ¨¡å‹
    print(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {args.model_name}")
    model = get_model(args.model_name).to(device)

    # è¼‰å…¥ `.pth` æ¬Šé‡æª”æ¡ˆ
    print(f"ğŸ”„ è¼‰å…¥æ¬Šé‡: {args.model_path}")
    checkpoint = torch.load(args.model_path, map_location=device)

    # ç¢ºä¿ `model_state_dict` è¢«æ­£ç¢ºè¼‰å…¥
    if "model_state_dict" in checkpoint:
        state_dict = checkpoint["model_state_dict"]
    else:
        state_dict = checkpoint  # ç›´æ¥è¼‰å…¥

    # è®€å– class_to_idx.json ä¸¦å»ºç«‹ idx_to_class
    print("ğŸ“‚ è®€å–é¡åˆ¥æ˜ å°„æ–‡ä»¶...")
    # è®€å– class_to_idx.json ä¸¦è¼‰å…¥é¡åˆ¥å°æ‡‰è¡¨
    with open("class_to_idx.json", "r") as f:
        class_to_idx = json.load(f)  # é¡åˆ¥åç¨± â†’ æ•¸å­—ç´¢å¼• (e.g., "brush_hair": 0)

    # åè½‰å­—å…¸ï¼Œä½¿æˆ‘å€‘å¯ä»¥é€éç´¢å¼•æ‰¾åˆ°å°æ‡‰çš„é¡åˆ¥åç¨±
    idx_to_class = {}
    for class_name, class_index in class_to_idx.items():
        idx_to_class[class_index] = class_name  # è½‰æ›ç‚º (ç´¢å¼• â†’ é¡åˆ¥åç¨±)

    # å¿½ç•¥ä¸åŒ¹é…çš„éµå€¼
    try:
        model.load_state_dict(state_dict, strict=args.strict)
        print("âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹æ¬Šé‡ï¼")
    except RuntimeError as e:
        print(f"âš ï¸ è­¦å‘Šï¼šè¼‰å…¥æ¨¡å‹æ¬Šé‡æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œå˜—è©¦ `strict=False` æ–¹å¼è¼‰å…¥...\n{e}")
        model.load_state_dict(state_dict, strict=False)

    # å½±ç‰‡é è™•ç†
    print(f"ğŸ¬ æ­£åœ¨è™•ç†å½±ç‰‡: {args.video_path}")
    video_tensor = preprocess_video(args.video_path).to(device)

    # é€²è¡Œæ¨ç†
    print("ğŸ” é€²è¡Œå½±ç‰‡åˆ†é¡...")
    top_class_names, top_probs = predict(model, video_tensor, idx_to_class, top_k=args.top_k)

    # é¡¯ç¤ºçµæœ
    print("\nğŸ“Š Top-{} åˆ†é¡çµæœï¼š".format(args.top_k))
    for i in range(args.top_k):
        print(f"{i + 1}. é¡åˆ¥: {top_class_names[i]}, æ©Ÿç‡: {top_probs[i]:.4f}")


if __name__ == "__main__":
    main()
